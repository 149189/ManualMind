# =============================================================================
# ManualMind RAG System - env.example (no secrets)
# =============================================================================

# ------------------------
# API & SERVER CONFIGURATION
# ------------------------
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_RELOAD=false
# Set this to a strong secret in your deployment (DO NOT commit the real key)
API_SECRET_KEY=
MAX_FILE_SIZE=52428800  # 50MB in bytes
# If you use HF Hub, place your token here (leave blank for local-only setups)
HUGGING_FACE_HUB_TOKEN=

# ------------------------
# LLM CONFIGURATION
# ------------------------
# Options: http, local, mock
LLM_BACKEND=http
# URL of your local model server (e.g. TGI, Ollama, etc.)
LLM_API_URL=http://localhost:8080
LLM_MODEL_NAME=llama2
LLM_TIMEOUT=60.0
LLM_MAX_TOKENS=512
LLM_TEMPERATURE=0.1
LLM_DO_SAMPLE=true

# ------------------------
# EMBEDDING CONFIGURATION
# ------------------------
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DEVICE=auto  # auto, cuda, cpu
EMBEDDING_BATCH_SIZE=32
EMBEDDING_CACHE_ENABLED=true

# ------------------------
# VECTOR STORE CONFIGURATION
# ------------------------
INDEX_PATH=./data/index/faiss.index
META_PATH=./data/index/meta.jsonl
DB_PATH=./data/index/metadata.db
INDEX_DIMENSION=384
FAISS_INDEX_TYPE=FlatL2  # Options: FlatL2, IVF, HNSW

# ------------------------
# INGESTION CONFIGURATION
# ------------------------
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MIN_CHUNK_LENGTH=50
MAX_CHUNK_LENGTH=2000
REMOVE_HEADERS=true
REMOVE_FOOTERS=true
REMOVE_PAGE_NUMBERS=true

# ------------------------
# SEARCH CONFIGURATION
# ------------------------
DEFAULT_TOP_K=5
MAX_TOP_K=20
MIN_SIMILARITY_SCORE=0.0
SIMILARITY_METRIC=cosine  # cosine, l2, ip

# ------------------------
# DATABASE CONFIGURATION
# ------------------------
# Example SQLite URL for local dev
DATABASE_URL=sqlite:///./data/manualmind.db
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_ECHO=false

# ------------------------
# CACHE CONFIGURATION
# ------------------------
# If you need redis, set REDIS_URL; leave blank to disable
REDIS_URL=redis://localhost:6379/0
CACHE_TTL=3600
CACHE_ENABLED=true

# ------------------------
# LOGGING CONFIGURATION
# ------------------------
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=./logs/manualmind.log
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_ROTATION=10MB
LOG_RETENTION=7  # days

# ------------------------
# SECURITY CONFIGURATION
# ------------------------
# Comma-separated list of allowed origins for CORS in dev
CORS_ORIGINS=http://localhost:3000,http://localhost:3001
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=3600
# Which header the API expects for the API key; either "Authorization" or a custom header like "X-API-Key"
API_KEY_HEADER=X-API-Key

# ------------------------
# STORAGE CONFIGURATION
# ------------------------
UPLOAD_DIR=./data/uploads
TEMP_DIR=./data/temp
MAX_TEMP_FILES=100
TEMP_FILE_AGE=24  # hours

# ------------------------
# MONITORING CONFIGURATION
# ------------------------
PROMETHEUS_ENABLED=true
HEALTH_CHECK_INTERVAL=30
METRICS_PORT=9090

# ------------------------
# PERFORMANCE CONFIGURATION
# ------------------------
WORKER_TIMEOUT=120
WORKER_GRACEFUL_TIMEOUT=30
MAX_CONCURRENT_INGESTIONS=3
MAX_CONCURRENT_QUERIES=10

# ------------------------
# LLM-SPECIFIC SETTINGS (optional)
# ------------------------
# For TGI
TGI_MAX_BATCH_SIZE=32
TGI_WAIT_FOR_MODEL=true
TGI_DO_SAMPLE=true

# For Ollama / vLLM tuning (optional)
OLLAMA_KEEP_ALIVE=5m
OLLAMA_NUM_CTX=4096
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_MODEL_LEN=4096

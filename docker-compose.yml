# C:\Users\kaust\OneDrive\Documents\GitHub\ManualMind\docker-compose.yml
services:
  manualmind-backend:
    build:
      context: ./rag-manuals # Change context to the rag-manuals directory
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - LLM_BACKEND=http
      - LLM_API_URL=http://llm-service:8080
      - LLM_MODEL_NAME=mistral-7b-instruct
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
      - INDEX_PATH=/data/faiss.index
      - META_PATH=/data/meta.jsonl
      - API_SECRET_KEY=${API_SECRET_KEY:-change-this-in-production}
      - LOG_LEVEL=INFO
      - CORS_ORIGINS=http://localhost:3000,http://localhost:3001
    volumes:
      - manualmind-data:/data
    depends_on:
      - llm-service
    restart: unless-stopped

  llm-service:
    image: ghcr.io/huggingface/text-generation-inference:latest
    command: ["--model-id", "mistralai/Mistral-7B-Instruct-v0.2"]
    ports:
      - "8080:80"
    environment:
      - MAX_INPUT_LENGTH=4096
      - MAX_BATCH_SIZE=32
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - llm-models:/data
    restart: unless-stopped

volumes:
  manualmind-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./rag-manuals/data
  llm-models:
